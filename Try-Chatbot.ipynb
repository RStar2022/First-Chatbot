{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "id": "skB6rUEpgQfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "id": "7ABqb7zRgVbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras"
      ],
      "metadata": {
        "id": "5ZXLES-MgX3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_preprocessing"
      ],
      "metadata": {
        "id": "X1kivOgDmjzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiGQyYIjgMC8"
      },
      "outputs": [],
      "source": [
        "\"ChatBot\"\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#Download NLTK data\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "#Load data\n",
        "import json\n",
        "#f = open(\"/home/user/Downloads/intents.json\")\n",
        "f = open(\"intents.json\")\n",
        "data = json.load(f)\n",
        "\n",
        "#Preprocess data\n",
        "import string\n",
        "words=[]\n",
        "classes=[]\n",
        "data_x=[]\n",
        "data_y = []\n",
        "ignore_words=[\"!\",\"?\"]\n",
        "for intent in data[\"intents\"]:\n",
        "    for pattern in intent[\"patterns\"]:\n",
        "        #Tokenize data\n",
        "        tokens = nltk.word_tokenize(pattern) #tokenize each pattern\n",
        "        words.extend(tokens) #and append tokens and words\n",
        "\n",
        "        data_x.append(pattern) #appending pattern to data_x\n",
        "        data_y.append(intent[\"tag\"]) #appending the associated tag to each pattern\n",
        "\n",
        "        if intent[\"tag\"] not in classes :\n",
        "          classes.append(intent[\"tag\"])\n",
        "\n",
        "# initializing lemmetizer to get stem words       \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "#lemmatize all words in the vocab and convert them to lowercase\n",
        "#if the words don't appear in punctuation\n",
        "words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in string.punctuation]\n",
        "#sorting the vocab and classes in alphabetical order and taking the # set to ensure no duplication error\n",
        "words = sorted(set(words))\n",
        "classes = sorted(set(classes))\n",
        "print(len(classes)*[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In this step convert text into numbers using the bag of words model\n",
        "# Create an array of number of size the same as the length of vocabulary lists.\n",
        "# Array = 1 if word is in pattern/tag being read(data_x) and 0 if absent\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "#Text to Numbers\n",
        "training = []\n",
        "out_empty = [0] * len(classes)\n",
        "# creating the bag of words model\n",
        "for idx, doc in enumerate(data_x):\n",
        "  bow = []\n",
        "  text = lemmatizer.lemmatize(doc.lower())\n",
        "  for word in words :\n",
        "    bow.append(1) if word in text else bow.append(0)\n",
        "    \n",
        "    #mark the index of class that the current pattern is associated to\n",
        "    output_row = list(out_empty)\n",
        "    output_row[classes.index(data_y[idx])]=1\n",
        "\n",
        "    #add the one hot encoded Bow and associated classes to training\n",
        "    training.append([bow, output_row])\n",
        "\n",
        "#shuffle the data and convert it to an array\n",
        "random.shuffle(training)\n",
        "training = np.array(training, dtype = object)\n",
        "#split the features and target labels\n",
        "train_x = np.array(list(training[:, 0]))\n",
        "train_y = np.array(list(training[:, 1]))"
      ],
      "metadata": {
        "id": "J2gwWL-jrXws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-cpu"
      ],
      "metadata": {
        "id": "ruHGOkpHhAYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Network Model\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]),),activation =\"relu\"))\n",
        "model.add(Dropout(0,5))\n",
        "model.add(Dense(64, activation =\"relu\"))\n",
        "model.add(Dropout(0,5))\n",
        "model.add(Dense(len(train_y[0]), activation = \"softmax\"))\n",
        "adam = tf.keras.optimizers.Adam(learning_rate=0.01, weight_decay = 1e-6)\n",
        "model.compile(loss =\"categorical_crossentropy\",\n",
        "              optimizer = adam,\n",
        "              metrics = [\"accuracy\"])\n",
        "print(model.summary())\n",
        "model.fit(x = train_x, y = train_y, epochs = 200, verbose = 1)"
      ],
      "metadata": {
        "id": "ajT4G8Rv2W5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the Input\n",
        "def clean_text(text):\n",
        "  tokens = nltk.word_tokenize(text)\n",
        "  tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "  return tokens\n",
        "\n",
        "\"\"\"\"Receives text(string) as an input and then tokenizes using word_tokenize\n",
        "Each token is then converted into ist lemmatizer.\n",
        "\"\"\"\n",
        "\n",
        "def bag_of_words(text, vocab):\n",
        "  tokens = clean_text(text)\n",
        "  bow = [0] * len(vocab)\n",
        "  for w in tokens :\n",
        "    for idx, word in enumerate(vocab):\n",
        "      if word == w :\n",
        "        bow[idx] = 1\n",
        "  return np.array(bow)\n",
        "\n",
        "\"\"\"Calls clean text func, converts the text into an array using the \n",
        "bow model using the input vocabulary, then return the same arrays\"\"\"\n",
        "\n",
        "def pred_class(text, vocab, labels):\n",
        "  bow = bag_of_words(text,vocab)\n",
        "  result = model.predict(np.array([bow]))[0] #Extracting probabilities\n",
        "  print(result)\n",
        "  thresh = 0.5\n",
        "  y_pred = [[indx, res] for indx, res in enumerate(result) if res> thresh]\n",
        "  y_pred.sort(key = lambda x : x[1], reverse = True) #sort values of probability in decreasing order\n",
        "  return_list = []\n",
        "  for r in y_pred :\n",
        "    print(r)\n",
        "    return_list.append(labels[r[0]]) #Contains labels(tags) for highest probability\n",
        "  return return_list\n",
        "  print(return_list)\n",
        "\n",
        "\"\"\"\n",
        "Takes text, vocab, and labels as input and returns a list that contains a tag\n",
        "corresponding to the highest probability\n",
        "\"\"\"\n",
        "\n",
        "def get_response(intents_list, intents_json):\n",
        "  if len(intents_list) == 0 :\n",
        "    result = \"sorry! I don't understand\"\n",
        "  else :\n",
        "    tag = intents_list[0]\n",
        "    list_of_intents = intents_json[\"intents\"]\n",
        "    for i in list_of_intents :\n",
        "      if i[\"tag\"] == tag :\n",
        "        result = random.choice(i[\"responses\"])\n",
        "        break\n",
        "  return result\n",
        "\n",
        "  \"\"\"\n",
        "  Takes the tag returned by previous func and uses it to randomly chocose a response\n",
        "  corresponding to the same tag in intent.json.\n",
        "  And if inten_list is empty, that is when the prob dont cross the threshold and will pas string \"Sorry\" \n",
        "  as ChatBot's response\n",
        "  \"\"\""
      ],
      "metadata": {
        "id": "3rY-S6F592rB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interacting with chatbot\n",
        "print(\"Press 0 if you don't want to chat with our Chatbot\")\n",
        "while True :\n",
        "  message = input(\"\")\n",
        "  if message == \"0\" :\n",
        "    break\n",
        "  intents = pred_class(message,words,classes)\n",
        "  result = get_response(intents, data)\n",
        "  print(result)"
      ],
      "metadata": {
        "id": "H2e1fYiQjh2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compile model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics =[\"accuracy\"])\n",
        "\n",
        "#Train model\n",
        "num_epochs = 50\n",
        "history = model.fit(training_data, training_labels, epochs = num_epochs, verbose =2)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def predict_answer(model,tokenizer, question):\n",
        "    #Preprocess question\n",
        "    question = preprocess(question)\n",
        "    #convert question to sequence\n",
        "    sequence = tokenizer.texts_to_sequences([question])\n",
        "    #Pad sequence\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding = padding_type, truncating = trunc_type)\n",
        "    #Predict answer \n",
        "    pred = model.predict(padded_sequence)[0]\n",
        "    #Get index of highest probability\n",
        "    idx = np.argmax(pred)\n",
        "    #Get answer\n",
        "    answer = tokenizer.index_word[idx]\n",
        "    return answer\n",
        "\n",
        "#Start chatbot\n",
        "while True :\n",
        "    question = input(\"You: \")\n",
        "    answer = predict_answer(model, tokenizer, question)\n",
        "    print(\"Chatbot :\", answer)"
      ],
      "metadata": {
        "id": "kOQaePreglja"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}